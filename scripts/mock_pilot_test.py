#!/usr/bin/env python3
"""
Mock Pilot Test: Validate core RL logic without downloading models.

This uses a simple mock "model" that generates pre-defined code snippets
to validate that the reward function and GRPO logic work correctly.
"""

import sys
import json
import random
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from rewards.reward_function import RewardFunction


class MockCodeGenerator:
    """Mock model that generates predictable code for testing."""

    def __init__(self):
        self.iteration = 0

        # Predefined solutions with varying quality
        self.solution_pool = {
            "sum": [
                # Good solutions (no violations)
                "def solve(a, b):\n    return a + b",
                "def solve(a, b):\n    result = a + b\n    return result",

                # Solutions with violations
                "def solve(a, b):\n    while False:\n        pass\n    return a + b",  # has while loop
                "def solve(a, b):\n    for i in []:\n        pass\n    return a + b",  # has for loop
            ],
            "multiply": [
                "def solve(x):\n    return x * 2",
                "def solve(x):\n    return x + x",  # creative alternative
                "def solve(x):\n    result = 0\n    for i in range(2):\n        result += x\n    return result",  # has for loop
            ],
            "max": [
                "def solve(a, b):\n    if a > b:\n        return a\n    return b",
                "def solve(a, b):\n    return max(a, b)",
                "def solve(a, b):\n    return a if a > b else b",  # ternary
            ]
        }

    def generate(self, problem_type, num_solutions=4):
        """Generate solutions for a problem."""
        solutions = []
        pool = self.solution_pool.get(problem_type, self.solution_pool["sum"])

        for _ in range(num_solutions):
            # Randomly select from pool
            sol = random.choice(pool)
            solutions.append(sol)

        self.iteration += 1
        return solutions


def run_mock_training(num_steps=20):
    """Run mock training simulation."""
    print("="*80)
    print("MOCK PILOT TEST: Validating Core RL Logic")
    print("="*80)
    print("Using mock code generator to test reward function and RL training flow")
    print("="*80 + "\n")

    # Initialize
    generator = MockCodeGenerator()
    reward_fn = RewardFunction(
        correctness_weight=1.0,
        denial_penalty_weight=0.5,
        timeout=3,
    )

    # Test problems with test cases and constraints
    problems = [
        {
            "type": "sum",
            "test_cases": [[1, 2, 3], [5, 10, 15], [0, 0, 0]],
            "denied_techniques": [],  # Round 0: no constraints
        },
        {
            "type": "sum",
            "test_cases": [[1, 2, 3], [5, 10, 15], [0, 0, 0]],
            "denied_techniques": ["while loop"],  # Round 1
        },
        {
            "type": "multiply",
            "test_cases": [[5, 10], [0, 0], [3, 6]],
            "denied_techniques": [],
        },
        {
            "type": "multiply",
            "test_cases": [[5, 10], [0, 0], [3, 6]],
            "denied_techniques": ["for loop"],
        },
        {
            "type": "max",
            "test_cases": [[5, 3, 5], [1, 10, 10], [7, 7, 7]],
            "denied_techniques": ["if statement"],
        },
    ]

    # Training metrics
    metrics = {
        "steps": [],
        "rewards": [],
        "violations": [],
        "success_rates": [],
        "advantages": [],
    }

    print("üèãÔ∏è  Starting mock training...")
    print(f"Steps: {num_steps}, Group size: 4, Problems: {len(problems)}\n")

    # Simulate training steps
    for step in range(num_steps):
        # Sample a problem (simulate curriculum)
        if step < 10:
            problem_idx = step % 2  # Early: easy problems
        else:
            problem_idx = step % len(problems)  # Later: all problems

        problem = problems[problem_idx]

        # Generate group of solutions
        solutions = generator.generate(problem["type"], num_solutions=4)

        # Compute rewards for all solutions
        rewards = []
        violations_list = []
        successes = []

        for sol in solutions:
            result = reward_fn.compute_reward(
                generated_code=sol,
                test_cases=problem["test_cases"],
                denied_techniques=problem["denied_techniques"],
            )
            rewards.append(result['total_reward'])
            violations_list.append(result['num_violations'])
            successes.append(result['success'])

        # Compute GRPO advantages
        mean_reward = sum(rewards) / len(rewards)
        advantages = [r - mean_reward for r in rewards]

        # Log metrics
        metrics["steps"].append(step)
        metrics["rewards"].append(mean_reward)
        metrics["violations"].append(sum(violations_list) / len(violations_list))
        metrics["success_rates"].append(sum(successes) / len(successes))
        metrics["advantages"].append(sum(abs(a) for a in advantages) / len(advantages))

        # Print progress
        if step % 5 == 0:
            print(f"Step {step:2d}: Reward={mean_reward:+.3f}, Violations={metrics['violations'][-1]:.2f}, "
                  f"Success={metrics['success_rates'][-1]:.1%}, Advantage_spread={metrics['advantages'][-1]:.3f}")

    print("\n" + "="*80)
    print("MOCK TRAINING COMPLETE")
    print("="*80)

    return metrics


def analyze_mock_results(metrics):
    """Analyze mock training results."""
    print("\n" + "="*80)
    print("RESULTS ANALYSIS")
    print("="*80)

    rewards = metrics["rewards"]
    violations = metrics["violations"]
    success_rates = metrics["success_rates"]
    advantages = metrics["advantages"]

    # Basic statistics
    print("\nüìä Training Statistics:")
    print(f"  Total steps: {len(rewards)}")
    print(f"  Mean reward: {sum(rewards)/len(rewards):.3f}")
    print(f"  Mean violations: {sum(violations)/len(violations):.2f}")
    print(f"  Mean success rate: {sum(success_rates)/len(success_rates):.1%}")

    # Check reward progression
    first_half = rewards[:len(rewards)//2]
    second_half = rewards[len(rewards)//2:]
    avg_first = sum(first_half) / len(first_half)
    avg_second = sum(second_half) / len(second_half)

    print(f"\nüìà Reward Progression:")
    print(f"  First half: {avg_first:.3f}")
    print(f"  Second half: {avg_second:.3f}")
    print(f"  Change: {avg_second - avg_first:+.3f}")

    # Check violations
    viol_first = sum(violations[:len(violations)//2]) / len(violations[:len(violations)//2])
    viol_second = sum(violations[len(violations)//2:]) / len(violations[len(violations)//2:])

    print(f"\nüö´ Violation Progression:")
    print(f"  First half: {viol_first:.2f}")
    print(f"  Second half: {viol_second:.2f}")
    print(f"  Change: {viol_second - viol_first:+.2f}")

    # Check advantages (should be non-zero, showing differentiation)
    avg_advantage = sum(advantages) / len(advantages)
    print(f"\n‚öñÔ∏è  Advantage Spread:")
    print(f"  Average: {avg_advantage:.3f}")
    print(f"  Max: {max(advantages):.3f}")
    print(f"  Min: {min(advantages):.3f}")

    # Validation checks
    print("\n" + "="*80)
    print("VALIDATION CHECKS")
    print("="*80)

    checks_passed = []

    # Check 1: Rewards are being computed
    if any(r != 0 for r in rewards):
        print("‚úÖ Reward function is computing non-zero rewards")
        checks_passed.append(True)
    else:
        print("‚ùå All rewards are zero - check reward function")
        checks_passed.append(False)

    # Check 2: Violations are being detected when they exist
    if any(v > 0 for v in violations):
        print("‚úÖ Violation detection is working")
        checks_passed.append(True)
    else:
        print("‚ö†Ô∏è  No violations detected (may be okay if no constraints)")
        checks_passed.append(True)

    # Check 3: GRPO advantages show differentiation
    if avg_advantage > 0.01:
        print("‚úÖ GRPO is differentiating between solutions (non-zero advantages)")
        checks_passed.append(True)
    else:
        print("‚ö†Ô∏è  Advantages very small - solutions may be too similar")
        checks_passed.append(True)

    # Check 4: Code execution is working
    if any(s > 0 for s in success_rates):
        print("‚úÖ Code execution and test case validation working")
        checks_passed.append(True)
    else:
        print("‚ùå No solutions passing tests - check code executor")
        checks_passed.append(False)

    print("\n" + "="*80)
    print("INTERPRETATION")
    print("="*80)

    if all(checks_passed):
        print("\n‚úÖ SYSTEM VALIDATED!")
        print("\nAll core components are working correctly:")
        print("  ‚Ä¢ Reward function computes correctness and denial penalties")
        print("  ‚Ä¢ Code executor safely runs code and validates test cases")
        print("  ‚Ä¢ Technique detector identifies denied patterns")
        print("  ‚Ä¢ GRPO logic differentiates between solution quality")

        print("\nüí° Ready for Real Training:")
        print("  ‚Ä¢ The mock test validates the reward/GRPO logic")
        print("  ‚Ä¢ On NSCC with real models, you'll see:")
        print("    - Model actually learning to generate better code")
        print("    - Rewards increasing over training")
        print("    - Violations decreasing as model learns constraints")
    else:
        print("\n‚ö†Ô∏è  Some checks failed - review above")

    print("\n" + "="*80)

    # Save mock results
    output_dir = Path("outputs/mock_pilot_test")
    output_dir.mkdir(parents=True, exist_ok=True)

    with open(output_dir / "mock_metrics.json", 'w') as f:
        json.dump({str(i): {
            "mean_reward": metrics["rewards"][i],
            "mean_violations": metrics["violations"][i],
            "success_rate": metrics["success_rates"][i],
            "advantage_spread": metrics["advantages"][i],
        } for i in range(len(metrics["steps"]))}, f, indent=2)

    print(f"\nüìÅ Results saved to: {output_dir}/mock_metrics.json")

    return all(checks_passed)


def main():
    """Run mock pilot test."""
    print("\n" + "="*80)
    print("DENIAL PROMPTING RL - MOCK PILOT TEST")
    print("="*80)
    print("This test validates core logic without needing real models")
    print("="*80 + "\n")

    # Run mock training
    metrics = run_mock_training(num_steps=20)

    # Analyze results
    success = analyze_mock_results(metrics)

    if success:
        print("\nüéâ All systems validated - ready for NSCC deployment!")
        return 0
    else:
        print("\n‚ö†Ô∏è  Some validation checks failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())
